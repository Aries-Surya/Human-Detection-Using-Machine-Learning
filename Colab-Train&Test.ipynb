{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Human-Detection-Using-Machine-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRc51USu2xht",
    "outputId": "f7fc6f04-a79c-469e-bc6f-538b5f9d57c8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5TSCJg8629l",
    "outputId": "69430b46-5908-495c-8f3b-df5f328f7726"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/data-set.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kx_KhOO27M1Y",
    "outputId": "e55e5bfe-7c7e-407c-ddd7-f444dae55934"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hxPl3z8CRgV",
    "outputId": "a64e553f-aa9e-449a-f616-566c07800ae7"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the content you want to write to the YAML file\n",
    "data = {\n",
    "    'path': '/content/dataset/data.yaml',\n",
    "    'train': '/content/dataset/train/images',\n",
    "    'val': '/content/dataset/valid/images',\n",
    "    'test': '/content/dataset/test/images',\n",
    "    'names': {\n",
    "        0: 'person'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Path to your YAML file\n",
    "yaml_file_path = '/content/dataset/data.yaml'\n",
    "\n",
    "# Write the content to the YAML file\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{yaml_file_path}' has been overwritten successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train the model using YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRTv04Cd8_HP",
    "outputId": "46bfe779-aed8-459d-c7f3-457f91bf00b0"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO # Load the YOLOv8 model\n",
    "model = YOLO('yolo11x-obb.pt')  # Choose a model variant\n",
    "# Train the model using the data.yaml configuration\n",
    "model.train(data='/content/dataset/data.yaml', epochs=15, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO # Load the YOLOv8 model\n",
    "model = YOLO('yolo11x-obb.pt')  # Choose a model variant\n",
    "dataset_path = \"/content/dataset/data.yaml\" # Choose the Dataset Path\n",
    "# Train the model using the data.yaml configuration\n",
    "train_results = model.train(\n",
    "    data=dataset_path,\n",
    "    epochs=50,          # More epochs for better accuracy\n",
    "    imgsz=640,         # Larger image size improves detection\n",
    "    device=\"cuda\",      # Use GPU in Colab\n",
    "    workers=8,         # More workers for faster data loading\n",
    "    batch=16,          # Larger batch size for GPU efficiency\n",
    "    augment=True,      # Enable data augmentation\n",
    "    lr0=0.0005,        # Lower learning rate for stability\n",
    "    patience=10,       # Early stopping if no improvement\n",
    "    optimizer=\"AdamW\", # AdamW optimizer for better training stability\n",
    "    save=True,         # Save the best model\n",
    "    project=\"/content/yolo_training_results\",  # Save outputs to Google Drive\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Image Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "gcjrhKp0Txyi",
    "outputId": "f9056bad-b6e8-4cd6-9f53-616accd455bb"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('/content/yolo_training_resultsruns/detect/train/weights/best.pt')\n",
    "\n",
    "# Load a test image\n",
    "image_path = '/content/dataset/26-08-2024/test/images/fsi122_jpg.rf.522dadebf50d11555d7317bd3f870330.jpg'  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform inference\n",
    "results = model(image)\n",
    "\n",
    "# Process and display the results\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    for box in boxes:\n",
    "        # Get the coordinates of the bounding box\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Get the label and confidence score\n",
    "        label = model.names[int(box.cls)]\n",
    "        confidence = box.conf[0]\n",
    "\n",
    "        # Display label and confidence on the image\n",
    "        cv2.putText(image, f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2_imshow(image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the result\n",
    "cv2.imwrite('output/output.jpg', image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Video Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
    "# Run inference on the video\n",
    "results = model.predict(source=\"Input/Vid/Input.mp4\", save=True, save_crop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw8JUkT6Dsvy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Output Rendering of Video's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdydmWHcDpor",
    "outputId": "e5f3c799-c11e-42ff-8082-83784a8c7269"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "model_path = r'/content/runs/detect/train/weights/best.pt'\n",
    "output_folder = r'/content/output'\n",
    "output_video_path = os.path.join(output_folder, 'annotated_output.mp4')\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_folde):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Load the trained model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# For recorded video input:\n",
    "video_source = r'/content/Video.mp4'\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "# Check if the video source opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source {video_source}\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "frame_number = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"End of video stream or failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Perform inference\n",
    "    results = model.predict(source=frame)\n",
    "\n",
    "    # Process results\n",
    "    annotated_frame = False  # Flag to track if any person was detected\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        names = result.names\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()  # Get bounding box coordinates\n",
    "                cls = int(box.cls[0])  # Get class index\n",
    "                conf = box.conf[0]  # Get confidence score\n",
    "                label = names[cls]\n",
    "\n",
    "                if label == 'person' and conf >= 0.3:  # Check if detected object is a person with a confidence score above a threshold\n",
    "                    annotated_frame = True  # Set the flag to True if a person is detected\n",
    "\n",
    "                    # Draw bounding boxes and labels\n",
    "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f'{label} {conf:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    if annotated_frame:\n",
    "        # Write the annotated frame to the video file\n",
    "        out.write(frame)\n",
    "\n",
    "    # Display the frame (optional)\n",
    "    # cv2.imshow('YOLO Detection', frame)\n",
    "\n",
    "    # Press 'q' to quit the video loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f'Annotated video saved at: {output_video_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CB5ENSWlQKA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Downlord the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJBMNwizT5td"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_directory(folder_path, output_path):\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, folder_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "# Path to the folder you want to zip\n",
    "folder_to_zip = '/content/output'\n",
    "# Path where you want to save the zip file\n",
    "zip_file_path = '/content/results.zip'\n",
    "\n",
    "zip_directory(folder_to_zip, zip_file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
